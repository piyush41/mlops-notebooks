{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving ML Models Using Web Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serving\n",
    "\n",
    "- Sharing results with others (team members, other business orgs, customers, web services, applications)\n",
    "- Batch approach: dump predictions to a database (quite popular)\n",
    "- Real-time approach: send a test feature vector and get back the prediction (the inference step happens just-in-time)\n",
    "\n",
    "### How to consume from prediction services?\n",
    "\n",
    "- Using web requests (e.g., using a JSON payload)\n",
    "\n",
    "### How to output predictions?\n",
    "\n",
    "- We will plan to set up a server to serve predictions\n",
    "  - It will respond to web requests (GET, POST)\n",
    "    - We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function).\n",
    "    - The environment from which we pass inputs may be very different from the environment where the prediction happens (e.g., different hardware).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Our Objective\n",
    "\n",
    "We will learn how to serve model predictions via the following steps:\n",
    "\n",
    " - 1. We will understand the key idea (mapping URL routes to functions) behind the `flask` web framework through an example flask app.\n",
    " - 2. We will use the `requests` module from a jupyter notebook (this is an example of a programmatic way to get any information from other machines on the internet). Alternatively, one can use commandline tools such as `curl` or commercial/GUI tools such as `postman` (these serve different needs of end users).\n",
    " - 3. Integrating the model with the app is relatively easy if the model can be read from disk. We will use the pytorch model with flask (see how to use `gunicorn` and Heroku PaaS in the exercises section) to set up a prediction server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making API calls\n",
    "\n",
    " - Most of the internet works via HTTP requests.\n",
    " - The key idea is that a requester (a client) will send some information to the server located by  unique address (the IP address).\n",
    " - The server in turn processes the request and sends back a response (whereever the client is).\n",
    " - There are various types of requests:\n",
    "   - GET: mostly used to access read-only data from the server\n",
    "   - POST: mostly used to modify some information on the server (e.g., new user registration)\n",
    "   - PUT\n",
    "\n",
    "Below is our first example of making a GET request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Response code: <Response [200]>\n",
      "Returned text:  {\"input1\":\"1\",\"input2\":\"4\",\"prediction1\":4.0,\"prediction2\":10.0}\n",
      "\n",
      "2\n",
      "Response code: <Response [200]>\n",
      "Returned text:  {\"input1\":\"2\",\"input2\":\"5\",\"prediction1\":6.0,\"prediction2\":12.0}\n",
      "\n",
      "3\n",
      "Response code: <Response [200]>\n",
      "Returned text:  {\"input1\":\"3\",\"input2\":\"6\",\"prediction1\":8.0,\"prediction2\":14.0}\n",
      "\n",
      "4\n",
      "Response code: <Response [200]>\n",
      "Returned text:  {\"input1\":\"4\",\"input2\":\"7\",\"prediction1\":10.0,\"prediction2\":16.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "for x in range(1,5):\n",
    "    print(x)\n",
    "    res = requests.get(f'http://127.0.0.1:5000/?x={str(x)}&y={str(x+3)}')\n",
    "\n",
    "    print('Response code:',res)\n",
    "    print('Returned text: ',res.text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"input1\":\"5\",\"input2\":\"3\",\"prediction1\":12.0,\"prediction2\":8.0}\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A status code of 200 means the server was able to respond as intended. A 4xx code means there was an issue with the client and a 5xx code means there was an issue with the server. We may face a lot of the latter codes when we try to deploy our models and we should learn to debug them properly (more on this later).\n",
    "\n",
    "The same request above can be made using a commandline utility found in Ubuntu/Debian and other linux distros:\n",
    "```bash\n",
    "curl -o output.json https://httpbin.org/get\n",
    "curl -o temp.html https://theja.org\n",
    "```\n",
    "\n",
    "Finally requests can also be made using more sophisticated programs such as [Postman](https://www.postman.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: http://127.0.0.1:5000/?x=5\n"
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:5000/?x=5&y=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
